% -*- root:main.tex -*-

\section{Implementing}

To obtain a high performance implementation, several of our most heavily used operations have been significantly optimized. In this section we highlight the most important ones. The most costly operation in \cite{DBLP:conf/tcc/FrederiksenJNT16} protocol is the matrix transpose that maps $m$ secret sharings of $n$ bit codewords to its transpose consisting of $n$ entries, each $m$ bits long. In order to perform this operation efficiently, the implementation employs optimizations originally developed for OT extension protocols. The main technique is to use special instructions available on modern Intel CPUs to accelerate the computation. The most notable of these instructions is \texttt{\_mm\_movemask\_epi8} which takes 16 bytes as input and returns a 16 bit number consisting of the least significant bit of each byte. By performing 8 of these operations interleaved with shifting the 16 bytes to the right allows for the efficient transpose of a $8\times 16$ bit matrix, requiring roughly $20\times$ fewer instructions over a standard approach. This optimization was first observed in the cryptography community by the authors of the OT extension protocol~\cite{DBLP:conf/crypto/KellerOS15} and later described in a related work~\cite{DBLP:conf/ccs/KellerOS16}. While implementing this work and the related libOTe~\cite{libOTe} project, subsequent improvements on this technique have been made. In particular, we employ Intel vector operations to allow 8 simultaneously \texttt{\_mm\_movemask\_epi8} instructions to be performed, resulting in even greater throughput.\footnote{Most modern Intel CPUs support 8 vector lanes.} In total these optimizations yield several orders of magnitude improvements over a naive implementation and were essential in obtaining high performance.


Another computationally intensive operation is the construction of codewords that are subsequently secret shared. The most straightforward way to construct codewords over $\mathbb{F}_2$ is by multiplying the plaintext word interpreted as a bit vector by a bit matrix, also known as the generator. One important observation made by \cite{DBLP:conf/tcc/FrederiksenJNT16} is that a linear code can be placed in systematic form where codewords are the input concatenated with a series of parity bits. As such, only the parity bits need to be computed using matrix multiplication. However, this computation can still dominate the running time if implemented naively. By naively, we mean to test each bit of the input and then add the corresponding row of the generator matrix to a running sum if the bit is set. When performing this operation for  millions of bits, it induces significant performance penalties due to the CPU not being able to effectively perform branch predication on the ``random" input bits, thus resulting in very poor instruction pipelining.

Intuitively, we overcome this challenge by performing this conditional operation on a whole byte as opposed to single bits. This has several advantages: 1) fewer conditionals are performed. 2) branch prediction is much more effective due to being conditioned on a whole byte.  3) The resulting operations are compatible with instruction vectorization. To enable this optimization, a pre-processing on the generator matrix itself is performed where for each set of 8 rows of the matrix, all 256 possible additive combinations are constructed and stored. The original matrix multiplication can then be reduced to using the bytes of the input to index into the corresponding row of this expended generator matrix. To then achieve optimal performance, this indexing operation can be vectorized, allowing 8 indexing operations to be performed in parallel.

The implementation as is directly supports commitments to random values. These values are obtained by first performing $n$ so called seed OTs on random and short strings. We then use these these short strings to generate the two matrices of secret shared codewords. In particular, the seed OTs are used as seeds to such a pseudo-random number generator (PRNG) which is used to expand the seeds. Our final optimization which is widely known in the cryptography community is to use the AES-NI instruction to implement such a PRNG. In all modern Intel CPUs, the AES-NI instruction set allows for fast encryption via special hardware support. Therefore, by using AES in counter-mode, we get a hardware accelerated PRNG. As with the previous optimizations, it is critical for performance that these AES operations be vectorized, meaning at least 8 blocks of output are computed in parallel.
%In this section, we can share the insight that were gained from implementing it. This includes the SSE instructions for fast trnaspose. Your trick for doing the consistency check with mul128 instructions, and my fast linear code optimization.

%In total, these optimizations improve the running time of the implementation by several orders of magnitude. When compared to the random oracle commitment scheme, where the computation is significantly more straight forward, these optimizations allow for an argument to be made that SplitCommit is an attractive alternative.